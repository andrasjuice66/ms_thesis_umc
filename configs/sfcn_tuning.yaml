# ────────────────────────────  DATA  ──────────────────────────── #
data:
  data_dir: "Data"               # root that contains all images
  train_csv: "brain_age_pred/data/labels/train_min.csv"
  val_csv:   "brain_age_pred/data/labels/val_min.csv"
  test_csv:  "brain_age_pred/data/labels/test_min.csv"

  image_key:  "image_path"
  age_key:    "age"
  weight_key: "sample_weight"             

  # loading / caching
  cache_dir: "data/cache"
  cache_mode: "memory"                     # memory | disk | none
  preload: true
  num_workers: 8
  pin_memory: false
  persistent_workers: true
  prefetch_factor: 4
  target_shape: [182, 218, 182]            # (D,H,W) after optional resize

# ───────────────────────────  MODEL  ──────────────────────────── #
model:
  type: "sfcn"                             # sfcn | resnet3d | efficientnet3d
  in_channels: 1
  dropout_rate: 0.5
  use_attention: false                   
  checkpoint: null #"checkpoints/sfcn_20250511_152522_epoch100.pt"  # path to checkpoint file

# ────────────────  DOMAIN-RANDOMISATION / AUGS  ───────────────── #
# -------------------------------------------------------------------------
# Domain-randomisation settings
# Every key here maps 1-to-1 to a keyword argument or probability override
# accepted by the new DomainRandomizer class.
# -------------------------------------------------------------------------
domain_randomization:

  # ── global toggles ────────────────────────────────────────────────────
  augmentation_strength: "medium"        # optional meta-flag (not parsed)
  use_torchio: false                     # enable heavy TorchIO artefacts
  output_shape:        [182, 218, 182]   # None ⇒ keep full FoV

  # ── per-transform probabilities (omit to use defaults) ───────────────
  transform_probs:
    flip:        0.50
    affine:      0.80
    elastic:     0.40     # TorchIO elastic deformation
    contrast:    0.60
    gamma:       0.50
    blur:        0.40
    bias:        0.50
    scale_int:   0.40     # RandScaleIntensityd
    shift_int:   0.40     # RandShiftIntensityd
    hist_shift:  0.30     # RandHistogramShiftd
    noise:       0.40     # Gaussian noise
    rician:      0.30     # Rician noise
    gibbs:       0.30     # Gibbs ringing
    resolution:  0.50     # RandomResolutionD
    coarse_do:   0.0     # RandCoarseDropoutd
    spike:       0.20     # TorchIO spike artefact
    ghost:       0.30     # TorchIO ghosting
    crop:        1.00     # RandSpatialCropd

  # ── geometric parameters ─────────────────────────────────────────────
  enable_spatial:  true
  scaling_range:   [0.5, 1.5]            # uniform per-axis scale
  rotation_range:  15.0                  # degrees (±)
  shearing_bounds: 0.05                  # shear factor
  nonlinear_std:   5.5                   # (if you extend elastic)

  # ── intensity parameters ─────────────────────────────────────────────
  contrast_range:  [0.6, 1.6]            # RandAdjustContrast & ScaleIntensity
  log_gamma_std:   0.20                  # RandGammaD (log-space σ)
  bias_field_range: [0.0, 0.4]           # RandBiasFieldd
  shift_intensity_offsets: [-0.1, 0.1]   # RandShiftIntensityd
  histogram_num_control_points: [5, 10]  # RandHistogramShiftd

  # noise
  noise_std_range:   [0.0, 0.07]         # Gaussian σ
  rician_std_range:  [0.0, 0.07]         # Rician σ
  gibbs_alpha_range: [0.0, 1.0]          # Gibbs α (0→none, 1→strong)

  # ── resolution / simulation ──────────────────────────────────────────
  enable_simulation: true
  max_res_iso:       3.0                 # upper isotropic voxel-size (mm)
  resolution_range:  [1.0, 2.5]          # passed to RandomResolutionD
  coarse_dropout_size: [20, 20, 20]      # voxel size of cut-out cubes

  # ── TorchIO artefacts (only used if `use_torchio: true`) ─────────────
  enable_artifacts: true
  elastic_max_disp:   5.0                # voxels, RandomElasticDeformation
  spike_num_range:    [1, 6]
  spike_intensity_range: [0.1, 0.6]
  ghost_num_range:    [2, 10]

  # ── misc / bookkeeping ───────────────────────────────────────────────
  #max_augmentations_per_sample: 5        # (not implemented yet)

# ──────────────────────────  TRAINING  ────────────────────────── #
training:
  batch_size: 8
  epochs: 100

  # optimiser & LR
  learning_rate: 0.001
  weight_decay:  0.00001
  optimizer: "sgd"                       # adam | adamw | sgd | rmsprop | radam | novograd
  scheduler: "cosine"                      # cosine | plateau | onecycle | step | none
  scheduler_params:
    T_max: 100
    eta_min: 0.00001
    epochs: 100
    steps_per_epoch: 534

  # losses
  loss: "huber_mae"                        # mse | mae | huber | huber_mae | weighted_mse
  loss_params:
    delta: 1.0
    mae_weight: 0.5

  # misc
  early_stopping_patience: 8
  gradient_accumulation_steps: 1
  use_amp: true                            # mixed precision if CUDA present

# ────────────────────────────  OUTPUT  ────────────────────────── #
output:
  checkpoint_dir: "checkpoints"
  log_dir: "logs"
  experiment_name: null                   # auto-generate if left null

# ─────────────────────────────  W&B  ──────────────────────────── #
wandb:
  use_wandb: true                         # still requires $WANDB_API env-var
  project: "brainage-model-training"
  entity: null

# ───────────────────────── MISCELLANEOUS ───────────────────────── #
seed: 42
device: null                              # cuda if available, else cpu